{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Airflow?\n",
    "\n",
    "*Airflow* is a workflow management system (WMS) to handle simple to complex workflows. Below is a sample workflow to aggregate daily revenues from different Ad networks and provide a revenue forecast. Such workflows can be handled by Airflow.\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1382/1*ytMWtgd5h-1EiGiCe_D3ew.png\" width=\"750\">\n",
    "\n",
    "\n",
    "[Reference](https://towardsdatascience.com/why-quizlet-chose-apache-airflow-for-executing-data-workflows-3f97d40e9571)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use Cases \n",
    "\n",
    "* Extract Transform Load (ETL) jobs - extracting data from multiple sources, transforming for analysis, and loading it into another data store\n",
    "* Machine Learning pipelines\n",
    "* Data warehousing\n",
    "* Automated testing\n",
    "* Performing data backups\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Core Concepts\n",
    "\n",
    "## Airflow DAG\n",
    "DAG is short for *Directed Acyclic Graph*. It is a collection of tasks you want to run. DAG doesn't do any processing itself. It's job is to make sure that tasks are done at the right time and in the right order. Airflow DAGs are defined in standard Python files and in general one DAG file should correspond to a single workflow.\n",
    "\n",
    "<img src=\"https://www.polidea.com/static/bce5fcc8a3c0ead34ab459d243a26349/331ea/image2.png\"></img>\n",
    "\n",
    "\n",
    "\n",
    "## Operators\n",
    "While DAGs describe how to run a workflow, *Operators* determine what actually gets done. An operator describes a single task in a workflow. \n",
    "\n",
    "**Note**: Operators usually stand on their own and don't need to share resources with any other operators. If two operators need to share information, like a filename or small amount of data, consider combining them into a single operator.\n",
    "\n",
    "### Types of Operators\n",
    "\n",
    "* BashOperator       - executes a bash command\n",
    "* PythonOperator     - calls an arbitrary Python function\n",
    "* EmailOperator      -   sends an email\n",
    "* SimpleHttpOperator - sends a HTTP request\n",
    "* MySqlOperator, SqliteOperator, PostgresOperator etc. - executes a SQL command\n",
    "* **Sensor** - waits for a certain time, file, database row, S3 key\n",
    "* [Lot more ..](https://airflow.apache.org/_api/airflow/operators/index.html)\n",
    "\n",
    "## Tasks\n",
    "Once an operator is instaniated, it is referred as **task**. The instantiation defines specific values when calling an abstract operator, and the parameterized task becomes a node in a DAG.\n",
    "\n",
    "## Task Instances\n",
    "A task instance represents a specific run of a task and is characterized as the combination of DAG, a task, and a point in time. Task instances can have the below stages:\n",
    "<img src=\"https://airflow.apache.org/_images/task_lifecycle.png\"></img>\n",
    "\n",
    "[Reference](https://airflow.apache.org/concepts.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Core Benefits\n",
    "\n",
    "* Smart Scheduling\n",
    "* Depedency Management\n",
    "* Resilience\n",
    "* Scaleability\n",
    "* Flexibility\n",
    "* Monitoring & Interaction\n",
    "* Programmatic Pipeline Definition"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
